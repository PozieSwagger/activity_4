install.packages("rvest")
install.packages("httr")
# Load the required libraries
library(rvest)
library(httr)
# Specify the URL of the website to scrape
url <- "https://books.toscrape.com/"
# Send an HTTP GET request to fetch the website's content
response <- httr::GET(url)
# Parse the HTML content
parsed_html <- read_html(response)
# Extract the relevant data using CSS selectors
book_titles <- parsed_html %>% html_nodes(".product_pod h3 a") %>% html_attr("title")
prices <- parsed_html %>% html_nodes(".price_color") %>% html_text()
# Combine the extracted data into a data frame
scraped_data <- data.frame(Book_Title = book_titles, Price = prices)
# Print the scraped data
print(scraped_data)
library(rvest)
library(httr)
#Specify the URL of the website to scrape
url <- "https://www.d20pfsrd.com/classes/hybrid-classes/arcanist/"
#Send an HTTP GET request to fetch the website's content
response <- httr::GET(url)
#Parse the HTML content using the "rvest" package
parsed_html <- read_html(response)
library(rvest)
library(httr)
#Specify the URL of the website to scrape
url <- "https://www.d20pfsrd.com/classes/hybrid-classes/arcanist/"
#Send an HTTP GET request to fetch the website's content
response <- httr::GET(url)
#Parse the HTML content using the "rvest" package
parsed_html <- read_html(response)
library(rvest)
library(httr)
#Specify the URL of the website to scrape
url <- "https://www.d20pfsrd.com/classes/hybrid-classes/arcanist/"
#Send an HTTP GET request to fetch the website's content
response <- httr::GET(url)
#Parse the HTML content using the "rvest" package
parsed_html <- read_html(url)
#Extract the relevant data using CSS selectors
tables <- html_table(html_nodes(parsed_html, "table"))
df <- tables[[1]]
print(df)
View(tables)
View(df)
library(rvest)
library(httr)
#Specify the URL of the website to scrape
url <- "https://www.d20pfsrd.com/classes/hybrid-classes/arcanist/"
#Send an HTTP GET request to fetch the website's content
response <- GET(url)
#Parse the HTML content using the "rvest" package
parsed_html <- read_html(response)
install.packages("dplyr")
library(dplyr)
#Using the build feature
head(mtcars)
#Min-Max
min_val <- min(mtcars$hp)
max_val <- max(mtcars$hp)
mtcars$hp_normalized <- (mtcars$hp - min_val) / (max_val - min_val)
max_abs_value <- max(abs(mtcars$wt))
j <- nchar(as.integer(max_abs_value))
mtcars$wt_normalized <- mtcars$wt / (10^j)
summary(select(mtcars, hp, hp_normalized, wt, wt_normalized))
library(dplyr)
mpgCentered = scale(mtcars$mpg)
logTransformed = log(mpgCentered, base = exp(10))
summery(select(mtcars, mpg, logTransformed))
library(dplyr)
mpgCentered = scale(mtcars$mpg)
logTransformed = log(mpgCentered, base = exp(10))
summary(select(mtcars, mpg, logTransformed))
library(dplyr)
mtcars$mpgCentered = scale(mtcars$mpg)
mtcars$mpgCentered = log(mtcars$mpgCentered, base = exp(10))
summary(select(mtcars, mpg, mtcars$mpgCentered))
View(logTransformed)
View(mpgCentered)
library(dplyr)
mtcars$mpgCentered = scale(mtcars$mpg)
mtcars$mpglog = log(mtcars$mpg, base = exp(10))
summary(select(mtcars, mpg, mpgCentered, mpglog))
# Clear workspace
rm(list=ls())
# Create sample data
data <- c(12, 25, 33, 42, 15, 28, 47, 49, 5, 35)
mean_value <- mean(data)
print(paste("Mean:", mean_value))
median_value <- median(data)
print(paste("Median:", median_value))
std_dev <- sd(data)
print(paste("Standard Deviation:", std_dev))
variance <- var(data)
print(paste("Variance:", variance))
range_value <- range(data)
print(paste("Range: ", range_value[1], "to", range_value[2]))
IQR_value <- IQR(data)
print(paste("IQR:", IQR_value))
CV <- (std_dev/mean_value) * 100
print(paste("Coefficient of Variation (%):", CV))
install.packages("reshape2")
library(reshape2)
tutoring_data <- data.frame(
No_Tutoring = c(62, 63, 65, 66, 62, 63, 64, 62, 63, 66),
One_Tutoring = c(70, 71, 71, 72, 69, 70, 70, 71, 72, 73),
Two_Tutoring = c(78, 79, 79, 76, 78, 78, 77, 78, 79, 80)
)
# Basic statistics
summary(tutoring_data)
sapply(tutoring_data, sd)  # standard deviation for each column
# t-test between No_Tutoring and One_Tutoring
ttest_result <- t.test(tutoring_data$No_Tutoring, tutoring_data$One_Tutoring)
# Display the result
print(ttest_result)
# Interpretation:
# If p-value is < 0.05, it suggests that attending one tutoring session leads to a statistically significant improvement in scores compared to not attending.
long_data <- melt(tutoring_data)
# Perform ANOVA
anova_result <- aov(value ~ variable, data=long_data)
summary(anova_result)
# If the p-value from the ANOVA test is < 0.05, it indicates a significant difference among the groups.
# We can perform a post-hoc test to see which specific groups differ.
# Access the first element of the ANOVA summary list
anova_table <- summary(anova_result)[[1]]
# Extract the p-value for the 'variable' factor
p_value <- anova_table["variable", "Pr(>F)"]
# Print the p-value to confirm
print(p_value)
if (p_value < 0.05) {
pairwise_result <- pairwise.t.test(long_data$value, long_data$variable, p.adj = "bonferroni")
print(pairwise_result)
}
# Clear workspace
rm(list=ls())
# Create sample data
data <- c(12, 25, 33, 42, 15, 28, 47, 49, 5, 35)
mean_value <- mean(data)
print(paste("Mean:", mean_value))
median_value <- median(data)
print(paste("Median:", median_value))
std_dev <- sd(data)
print(paste("Standard Deviation:", std_dev))
variance <- var(data)
print(paste("Variance:", variance))
range_value <- range(data)
print(paste("Range: ", range_value[1], "to", range_value[2]))
IQR_value <- IQR(data)
print(paste("IQR:", IQR_value))
CV <- (std_dev/mean_value) * 100
print(paste("Coefficient of Variation (%):", CV))
study_data <- data.frame(
No_Session = c(65, 67, 70, 72, 69, 68, 66, 67, 71, 73),
One_Session = c(77, 78, 78, 79, 76, 77, 77, 78, 79, 80),
Two_Session = c(85, 86, 87, 84, 85, 85, 86, 87, 88, 89)
)
mean_value <- mean(study_data)
print(paste("Mean:", mean_value))
median_value <- median(study_data)
study_data <- data.frame(
No_Session = c(65, 67, 70, 72, 69, 68, 66, 67, 71, 73),
One_Session = c(77, 78, 78, 79, 76, 77, 77, 78, 79, 80),
Two_Session = c(85, 86, 87, 84, 85, 85, 86, 87, 88, 89)
)
print("No Sessions")
mean_value <- mean(study_data$No_Session)
print(paste("Mean:", mean_value))
median_value <- median(study_data$No_Session)
print(paste("Median:", median_value))
std_dev <- sd(study_data$No_Session)
print(paste("Standard Deviation:", std_dev))
print("Two Sessions")
mean_value <- mean(study_data$Two_Session)
print(paste("Mean:", mean_value))
median_value <- median(study_data$Two_Session)
print(paste("Median:", median_value))
std_dev <- sd(study_data$Two_Session)
print(paste("Standard Deviation:", std_dev))
ttest_result <- t.test(study_data$No_Session, study_data$Two_Session)
print(ttest_result)
install.packages("arules")
install.packages("arulesViz")
library(arules)
library(arulesViz)
#Data for the Lab
data("Groceries") inspect(Groceries[1:5])
library(arules)
library(arulesViz)
#Data for the Lab
data("Groceries")
inspect(Groceries[1:5])
#Applying the Apriori Algorithm
rules <- apriori(Groceries, parameter = list(support = 0.001, confidence = 0.8))
#Examining the Results
summary(rules)
inspect(rules[1:5])
#Sorting and Filtering
rules_sorted <- sort(rules, by="confidence", decreasing=TRUE)
inspect(rules_sorted[1:5])
herb_rules <- subset(rules_sorted, rhs %in% "herbs")
inspect(herb_rules)
#Visualizing
plot(rules_sorted[1:20], method="graph")
#Reviewing
inspect(subset(rules_sorted, lift > 6)[1:5])
install.packages("rpart")
install.packages("caret")
install.packages("rpart.plot")
library(rpart.plot)
library(rpart)
library(caret)
# The iris dataset is built into R
data(iris)
head(iris)
# Summary of the dataset
summary(iris)
# Visualize the data
pairs(~Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris, col = iris$Species)
# Set a seed for reproducible results
set.seed(123)
# Split the data into training and test sets
index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
trainData <- iris[index,]
testData <- iris[-index,]
# Train the decision tree model
irisTree <- rpart(Species ~ ., data = trainData, method = "class")
# Visualize the decision tree
library(rpart.plot)
rpart.plot(irisTree)
# Make predictions on the test data
predictions <- predict(irisTree, newdata = testData, type = "class")
# Evaluate the model's accuracy
confusionMatrix <- confusionMatrix(predictions, testData$Species)
print(confusionMatrix)
# Prune the decision tree if necessary
prunedTree <- prune(irisTree, cp = irisTree$cptable[which.min(irisTree$cptable[,"xerror"]),"CP"])
# Visualize the pruned tree
rpart.plot(prunedTree)
# Make predictions with the pruned tree and evaluate again
prunedPredictions <- predict(prunedTree, newdata = testData, type = "class")
prunedConfusionMatrix <- confusionMatrix(prunedPredictions, testData$Species)
print(prunedConfusionMatrix)
setwd("D:/Program Files (x86)/School Code/activity_4")
# Load necessary libraries
library(tidyverse)
library(igraph)
library(tidytext)
library(tm)
library(widyr)
# Load the dataset
data <- read.csv("nobel-prize-winners-1.csv", stringsAsFactors = FALSE)
# Data Cleaning
# Remove empty rows and duplicates
data <- data %>% distinct() %>% na.omit(subset = c("motivation"))
# Standardize text columns
data$name <- str_trim(data$name)
data$category <- str_to_lower(data$category)
data$motivation <- str_to_lower(data$motivation)
# Convert date columns to Date format
data$born <- as.Date(data$born, format="%Y-%m-%d")
data$died <- as.Date(data$died, format="%Y-%m-%d")
# Network Text Analysis
# Tokenization
tokens <- data %>%
unnest_tokens(word, motivation) %>%
anti_join(stop_words, by="word") # Remove common stop words
# Create word co-occurrence matrix
word_pairs <- tokens %>%
pairwise_count(word, name, sort = TRUE)
# Build network graph
graph <- graph_from_data_frame(word_pairs, directed = FALSE)
# Plot the network
plot(graph, vertex.size=4,
vertex.color = "#0046B8",
vertex.label.color = 'lightgray',
vertex.label.cex = 0.5,
edge.arrow.size = 0.5,
edge.color = '#272727',
asp = 0)
